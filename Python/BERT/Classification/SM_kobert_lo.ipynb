{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"name":"SM_kobert_lo.ipynb","provenance":[],"collapsed_sections":["k4acOBfeMxsY","f1fCTS6IhTjy"],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5cc1743aa3434124bed5d7ab35cad595":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_91c7344cbca946878c062a0d3043998e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_deebc0fbb9ed41a9824d854c9c4c7cb3","IPY_MODEL_6a7d4e6a693748099ccbba0261b02857"]}},"91c7344cbca946878c062a0d3043998e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"deebc0fbb9ed41a9824d854c9c4c7cb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ac5a8a97b6214425960204af275777e4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":371391,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":371391,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fb4cddccb4d64170be8519a1ba81d223"}},"6a7d4e6a693748099ccbba0261b02857":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f708e4764e140cdac7ea6450f6b7706","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 371k/371k [00:01&lt;00:00, 329kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f76dc302f3a4dac945ea1b3cac07895"}},"ac5a8a97b6214425960204af275777e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fb4cddccb4d64170be8519a1ba81d223":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f708e4764e140cdac7ea6450f6b7706":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f76dc302f3a4dac945ea1b3cac07895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f88d2ee23d641899df92891783901a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b46040c2e87847d3b049b3541c4062c2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_25bc2bcbd5ba476fb41a93ba128779ab","IPY_MODEL_6a96f3e6695c4db09c32e261d7f9bd6e"]}},"b46040c2e87847d3b049b3541c4062c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25bc2bcbd5ba476fb41a93ba128779ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3df54073a9264de4a267dbee3b38754f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":77779,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":77779,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2ddf98b2d13448e89caf08584989901"}},"6a96f3e6695c4db09c32e261d7f9bd6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e4968cac9c194759bff8bcf7b9b7ba02","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 77.8k/77.8k [00:00&lt;00:00, 155kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c2c0877f4f647809fcdc28ed36ff693"}},"3df54073a9264de4a267dbee3b38754f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e2ddf98b2d13448e89caf08584989901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4968cac9c194759bff8bcf7b9b7ba02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0c2c0877f4f647809fcdc28ed36ff693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8b9429b4cb24fd9affa0f08a091f39d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_443fbe3d93fe42d5988e6131350767a7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1ddff6a093a47a499e1fe2a780d8c24","IPY_MODEL_6b5ef69ea7ff4adf87c9d96a3c0e7c6b"]}},"443fbe3d93fe42d5988e6131350767a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1ddff6a093a47a499e1fe2a780d8c24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7eedf5cb4881413a8dd0cb9307d15e84","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":426,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":426,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cc1eb7634a24666969dbf227f9dfb5b"}},"6b5ef69ea7ff4adf87c9d96a3c0e7c6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3b2825cda536495b8ebe6b2d9e4774d0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426/426 [00:13&lt;00:00, 30.8B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a7ba6ca652c64e3c8f5d271f73fb3bdd"}},"7eedf5cb4881413a8dd0cb9307d15e84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2cc1eb7634a24666969dbf227f9dfb5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b2825cda536495b8ebe6b2d9e4774d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a7ba6ca652c64e3c8f5d271f73fb3bdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"75650f1c394f4223bbc4e7bfbebcb436":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a892d0eb4816448b9cf42662149df689","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_89fc4e5d9de04bc9b49d81bdd01e3f9d","IPY_MODEL_7d2790361558455aa90d0e04b3b316e0"]}},"a892d0eb4816448b9cf42662149df689":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89fc4e5d9de04bc9b49d81bdd01e3f9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a3976184c7224e298f08e6400c07b5d9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":368792146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":368792146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_524da2107d4a448e878f19de6ebc77a4"}},"7d2790361558455aa90d0e04b3b316e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1072cc2b65d4490d9c904d74fca3f7ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 369M/369M [00:13&lt;00:00, 28.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1184a2ebed71498c9b93a907c9082c56"}},"a3976184c7224e298f08e6400c07b5d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"524da2107d4a448e878f19de6ebc77a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1072cc2b65d4490d9c904d74fca3f7ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1184a2ebed71498c9b93a907c9082c56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"guwmaR_mRqid","colab_type":"text"},"source":["## Reference\n","- Chris McCormick \n","- BERT Word Embeddings Tutorial<br>\n","http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/<br>\n","- colab<br>\n","https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=0NmMdkZO8R6q<br>\n","- Youtube<br>\n","https://www.youtube.com/watch?v=Hnvb9b7a_Ps&feature=youtu.be<br>\n","- BERT Fine-Tuning Tutorial with PyTorch<br>\n","https://mccormickml.com/2019/07/22/BERT-fine-tuning/<br>\n","- Naver<br>\n","https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP#scrollTo=P58qy4--s5_x<br>\n"]},{"cell_type":"markdown","metadata":{"id":"vWxS0vAyyomF","colab_type":"text"},"source":["#1.Set-up"]},{"cell_type":"markdown","metadata":{"id":"xuAV7_20xpGH","colab_type":"text"},"source":["##1.1. Using Colab GPU for Learning"]},{"cell_type":"markdown","metadata":{"id":"eDsqSILpyWdj","colab_type":"text"},"source":["- First of all, we need to select GPU in the runtime menu"]},{"cell_type":"code","metadata":{"id":"NLbis1TXxH7y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596981057105,"user_tz":-120,"elapsed":7585,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"2b663b3d-e203-4a4a-ff91-a70d6649dc05"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z5TSPL97uyQV","colab_type":"text"},"source":["- To check which GPU type you are using"]},{"cell_type":"code","metadata":{"id":"vdlCE7ptyTep","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596981069276,"user_tz":-120,"elapsed":4898,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"c8ae6119-c32b-44fb-d04e-3cddb58e6799"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j5fe7QLtyeXE","colab_type":"text"},"source":["##1.2. Install Hugging Face Library"]},{"cell_type":"code","metadata":{"id":"Zth7aTXgyzr2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"ok","timestamp":1596981081629,"user_tz":-120,"elapsed":9910,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"d8693bfe-d0e0-4e2a-c502-9f84c66b5bfa"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 37.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 28.5MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 38.6MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=467bd250729e7e500207ab58e4386842b47c555be7f337545341dc4ec63f3c8d\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2pM0vS6ly7IR","colab_type":"text"},"source":["#2.Corpus Data"]},{"cell_type":"markdown","metadata":{"id":"sFhEOKv5zHwv","colab_type":"text"},"source":["##2.1.Mount Google Drive to this Notebook instance."]},{"cell_type":"code","metadata":{"id":"cV_l7h1JzrYd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1596981106642,"user_tz":-120,"elapsed":22097,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"764e9f1b-37d9-4128-8f6d-7330beb80133"},"source":["# Mount Google Drive to this Notebook instance.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-NKAUp8Fz76_","colab_type":"text"},"source":["- To check the location of this drive"]},{"cell_type":"code","metadata":{"id":"IYw0wq1Fz6ob","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592569466721,"user_tz":-120,"elapsed":1827,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"c426b6c8-f63c-419b-ae75-386f4df9e767"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2Pe0wgHW08w0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596981112560,"user_tz":-120,"elapsed":3342,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"9861634a-9ed9-44d0-e4c8-2602322f76cf"},"source":["!ls drive/My\\ Drive/BERT/SM/KoBERT/Postposition/Data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy     test_Eyse.csv  train_Ey.csv    train_Lo.csv\n","test_Ey.csv  test_Lo.csv    train_Eyse.csv  t-SNE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Na7k3PYv1YFj","colab_type":"text"},"source":["##2.2.Install required packages"]},{"cell_type":"code","metadata":{"id":"_Xu3hHxfpQKF","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_WrdmSu0vHLI","colab_type":"text"},"source":["##2.3.Importing files to generate data"]},{"cell_type":"code","metadata":{"id":"qKRpTrahXUjq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1596981123892,"user_tz":-120,"elapsed":3927,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"c1e9e545-b922-4449-baf7-ea49133455ec"},"source":["fileDir = \"drive/My Drive/BERT/SM/KoBERT/Postposition/Data/test_Lo.csv\"\n","fr = open(fileDir, 'r')\n","contents= fr.readlines()\n","fr.close()\n","\n","test = pd.DataFrame(columns=('index', 'Label', 'Sentence'))\n","i = 0\n","index = \"\"\n","label = \"\"\n","sentence = \"\"\n","for content in contents:\n","    if i == 0:\n","        pass\n","    else:\n","        infos = content.split(\",\")\n","        index = infos[0]\n","        label = int(infos[1])\n","        sentence = infos[2].replace(\"\\n\",\"\")\n","        test.loc[i] = [index, label, sentence]\n","    i = i + 1\n","\n","print(test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["    index Label                         Sentence\n","1       0     0  이 넥타이는 수제품으로 우리나라에서는 하나밖에 없습니다.\n","2       1     0         모든 주장이 나름대로의 근거를 갖추고 있다.\n","3       2     0                 멍멍한 채로 시간이 흘러갔다.\n","4       3     0             만찬 시에는 반주로 마주앙이 나온다.\n","5       4     0          아내의 얼굴이 절망적으로 굳어지고 있었다.\n","..    ...   ...                              ...\n","463   462     5           무대 위로 무수한 꽃송이들이 날아오른다.\n","464   463     5       오른쪽 차창 밖으로는 끝없이 이어지는 바다였다.\n","465   464     5          철장 사이로 쳐다봐야 하는 형제들의 아픔!\n","466   465     5                  출근은 우선 공보부로 했다.\n","467   466     5              그분이 나를 문예반으로 끌어들였다.\n","\n","[467 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gsiTbOpCXm4w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1596981150189,"user_tz":-120,"elapsed":22931,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"2d65fde7-40a7-448b-998b-20f612fd8502"},"source":["fileDir = \"drive/My Drive/BERT/SM/KoBERT/Postposition/Data/train_Lo.csv\"\n","fr = open(fileDir, 'r')\n","contents= fr.readlines()\n","fr.close()\n","\n","train = pd.DataFrame(columns=('index', 'Label', 'Sentence'))\n","i = 0\n","index = \"\"\n","label = \"\"\n","sentence = \"\"\n","for content in contents:\n","    if i == 0:\n","        pass\n","    else:\n","        infos = content.split(\",\")\n","        index = infos[0]\n","        label = int(infos[1])\n","        sentence = infos[2].replace(\"\\n\",\"\")\n","        train.loc[i] = [index, label, sentence]\n","    i = i + 1\n","\n","print(train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["     index Label                        Sentence\n","1        0     0     그러나 진공 상태로 포장되어 있는 사랑이란 없다.\n","2        1     0             한 마디로 하여 정신병자인 것이다.\n","3        2     0         복수를 일본어로는 '가따끼우찌'라고 한다.\n","4        3     0                 마지막으로 \"기죽지 마라.\"\n","5        4     0  관세음 보살 불교에서 대자대비의 상징으로 받드는 보살.\n","...    ...   ...                             ...\n","4237  4236     5                 그다지 크지 않은 창너머로.\n","4238  4237     5                      펑크난 혼 사이로.\n","4239  4238     5       마카오 친구 집으로 돌아다님 86.5­87.9\n","4240  4239     5             양편으로 사람들이 주욱 서 있었다.\n","4241  4240     5                 밤에 벽으로 다니는 곤충).\n","\n","[4241 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vUDSHj-gflX_","colab_type":"text"},"source":["- 6 functions of Korean postposition '-Lo' <br>\n","0: FNS (final state)<br>\n","1: INS (instrumental)<br>\n","2: DIR (Direction)<br>\n","3: EFF (effector)<br>\n","4: CRT (criterion)<br>\n","5: LOC (location)<br>\n"]},{"cell_type":"code","metadata":{"id":"Ani8vg5e1d1i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1596981157046,"user_tz":-120,"elapsed":879,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"d7fa51e5-8769-4b4c-a7ae-1e75094fab06"},"source":["print(train[0:20])\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   index Label                        Sentence\n","1      0     0     그러나 진공 상태로 포장되어 있는 사랑이란 없다.\n","2      1     0             한 마디로 하여 정신병자인 것이다.\n","3      2     0         복수를 일본어로는 '가따끼우찌'라고 한다.\n","4      3     0                 마지막으로 \"기죽지 마라.\"\n","5      4     0  관세음 보살 불교에서 대자대비의 상징으로 받드는 보살.\n","6      5     0               일곱시간을 내리 입석으로 갔다.\n","7      6     0        그런 개는 천상 씨받이로나 쓰일 뿐이지요.\"\n","8      7     0           그 댓가로 돈을 요구한 병사도 있었다.\n","9      8     0                  조금쯤 슬퍼하는 표정으로.\n","10     9     0               「마지막으로 한말씀 더 드리죠.\n","11    10     0              「다시 그 사람 얘기로 돌아가죠.\n","12    11     0             마치 포장을 하듯 어둠으로 말이야.\n","13    12     0              우산 생산을 보기로 생각해 보자.\n","14    13     0                 지리여행으로 알맞은 곳이다.\n","15    14     0             위험사회는 참여로 탈출할 수 있다.\n","16    15     0                 조카가 혼잣말로 중얼거렸다.\n","17    16     0       언제나 저를 신중한 사람으로 믿고 있었거든요.\n","18    17     0               그때부터 나는 영옥이로 통했다.\n","19    18     0     정 사장은 공인회계사 출신으로 재무 관리에 밝다.\n","20    19     0             숨이 막힐 정도로 격렬한 키스였다.\n","(4241, 3)\n","(467, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w04WFX8RvZk0","colab_type":"text"},"source":["##2.4.Cleaning the corpus data; remove the punctuations"]},{"cell_type":"code","metadata":{"id":"t6owtcpwI_DB","colab_type":"code","colab":{}},"source":["#정제하기\n","\n","train['Sentence'] = train['Sentence'].str.replace(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\\\n\\t]+', \" \", regex=True)\n","test['Sentence'] = test['Sentence'].str.replace(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', \" \", regex=True)\n","train['Sentence'] = train['Sentence'].str.replace(r'\\t+', \" \", regex=True)\n","test['Sentence'] = test['Sentence'].str.replace(r'\\t+', \" \", regex=True)\n","train['Sentence'] = train['Sentence'].str.replace(r'[\\\\n]+',\" \", regex=True)\n","test['Sentence'] = test['Sentence'].str.replace(r'[\\\\n]+',\" \", regex=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kpKdAp8JM_L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1596981165702,"user_tz":-120,"elapsed":872,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"50d905db-67f3-4d46-fcc1-3d7365756271"},"source":["train.head(5)\n","test.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>Label</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>이 넥타이는 수제품으로 우리나라에서는 하나밖에 없습니다</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>모든 주장이 나름대로의 근거를 갖추고 있다</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>멍멍한 채로 시간이 흘러갔다</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>만찬 시에는 반주로 마주앙이 나온다</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>아내의 얼굴이 절망적으로 굳어지고 있었다</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  index Label                         Sentence\n","1     0     0  이 넥타이는 수제품으로 우리나라에서는 하나밖에 없습니다 \n","2     1     0         모든 주장이 나름대로의 근거를 갖추고 있다 \n","3     2     0                 멍멍한 채로 시간이 흘러갔다 \n","4     3     0             만찬 시에는 반주로 마주앙이 나온다 \n","5     4     0          아내의 얼굴이 절망적으로 굳어지고 있었다 "]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"I-TL0IAHmr7s","colab_type":"text"},"source":["#3.Data processing\n"]},{"cell_type":"markdown","metadata":{"id":"8gmNqmlJm7UV","colab_type":"text"},"source":["##3.1 Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"W6d0RYoawwYo","colab_type":"text"},"source":["- Load the Tokenization.py for KoBERT"]},{"cell_type":"code","metadata":{"id":"tDJ5WczMJa04","colab_type":"code","colab":{}},"source":["import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n","\n","from transformers import PreTrainedTokenizer\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n","\n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n","\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n","\n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n","\n","SPIECE_UNDERLINE = u'▁'\n","\n","\n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","\n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n","\n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n","\n","        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n","        self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n","\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","\n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n","\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n","\n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n","\n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n","\n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n","\n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n","\n","        return outputs\n","\n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n","\n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n","\n","        return new_pieces\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n","\n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n","\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A RoBERTa sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n","\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n","\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A BERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n","\n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n","\n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n","\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n","\n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","\n","        return out_vocab_model, out_vocab_txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQ9R86CdnIon","colab_type":"text"},"source":["- Convert each sentence to BERT format"]},{"cell_type":"code","metadata":{"id":"YCwY3Vj65ek9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1596981180733,"user_tz":-120,"elapsed":879,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"d715e96e-3883-468e-e585-b198b915eb23"},"source":["# 리뷰 문장 추출\n","sentences = train['Sentence']\n","# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 그러나 진공 상태로 포장되어 있는 사랑이란 없다  [SEP]',\n"," '[CLS] 한 마디로 하여 정신병자인 것이다  [SEP]',\n"," '[CLS] 복수를 일본어로는  가따끼우찌 라고 한다  [SEP]',\n"," '[CLS] 마지막으로  기죽지 마라  [SEP]',\n"," '[CLS] 관세음 보살 불교에서 대자대비의 상징으로 받드는 보살  [SEP]',\n"," '[CLS] 일곱시간을 내리 입석으로 갔다  [SEP]',\n"," '[CLS] 그런 개는 천상 씨받이로나 쓰일 뿐이지요  [SEP]',\n"," '[CLS] 그 댓가로 돈을 요구한 병사도 있었다  [SEP]',\n"," '[CLS] 조금쯤 슬퍼하는 표정으로  [SEP]',\n"," '[CLS] 「마지막으로 한말씀 더 드리죠  [SEP]']"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"VNy9ggncwnQb","colab_type":"text"},"source":["- Save the label data"]},{"cell_type":"code","metadata":{"id":"Gbg_Z4aKrpTI","colab_type":"code","colab":{}},"source":["# 라벨 추출\n","labels = train['Label'].values\n","labels_re = []\n","for label in labels:\n","  labels_re.append(label)\n","labels = labels_re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLB8aaY7w7dn","colab_type":"text"},"source":["- Check the KoBertTokenizer"]},{"cell_type":"code","metadata":{"id":"8eUZvWeSr7j4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183,"referenced_widgets":["5cc1743aa3434124bed5d7ab35cad595","91c7344cbca946878c062a0d3043998e","deebc0fbb9ed41a9824d854c9c4c7cb3","6a7d4e6a693748099ccbba0261b02857","ac5a8a97b6214425960204af275777e4","fb4cddccb4d64170be8519a1ba81d223","4f708e4764e140cdac7ea6450f6b7706","4f76dc302f3a4dac945ea1b3cac07895","4f88d2ee23d641899df92891783901a2","b46040c2e87847d3b049b3541c4062c2","25bc2bcbd5ba476fb41a93ba128779ab","6a96f3e6695c4db09c32e261d7f9bd6e","3df54073a9264de4a267dbee3b38754f","e2ddf98b2d13448e89caf08584989901","e4968cac9c194759bff8bcf7b9b7ba02","0c2c0877f4f647809fcdc28ed36ff693"]},"executionInfo":{"status":"ok","timestamp":1596981198940,"user_tz":-120,"elapsed":3070,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"b2a013b9-b446-4e95-dcdf-91caedec7877"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cc1743aa3434124bed5d7ab35cad595","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f88d2ee23d641899df92891783901a2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n","Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","[CLS] 그러나 진공 상태로 포장되어 있는 사랑이란 없다  [SEP]\n","['[CLS]', '▁그러나', '▁진', '공', '▁상태', '로', '▁포', '장', '되어', '▁있는', '▁사랑', '이란', '▁없다', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1O1deGkBsDuA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1596981202891,"user_tz":-120,"elapsed":964,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"3bb46437-9d93-47ef-eb7a-05e971b16813"},"source":[" # 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   2, 1199, 4360, 5452, 2680, 6079, 4856, 7178, 5894, 3860, 2590,\n","       7107, 3273,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"l7P-IfjxF2si","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1596981207401,"user_tz":-120,"elapsed":1256,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"c627e698-d24e-42a4-8e74-a6bcca22f84f"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])\n","print(labels)\n","print(input_ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n","[[   2 1199 4360 ...    0    0    0]\n"," [   2 4955 1907 ...    0    0    0]\n"," [   2 2404 6116 ...    0    0    0]\n"," ...\n"," [   2 1907 7495 ...    0    0    0]\n"," [   2 3214 7720 ...    0    0    0]\n"," [   2 2265 6896 ...    0    0    0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xFZBE1kBspIa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1596981210787,"user_tz":-120,"elapsed":876,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"39deb71a-7191-47ad-f801-95096c8ab1c9"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([   2, 3647, 7376, 7078,  517, 6819, 7074, 6586,    3,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(2)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n","tensor([   2, 4946, 4214, 4871, 6015, 6156, 2630, 5760, 1185, 7483, 7318, 3157,\n","           3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(3)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"65hn2YK2F9yb","colab_type":"code","colab":{}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uafSA5FZssF4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1596981224806,"user_tz":-120,"elapsed":852,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"debee370-cb15-422d-8bd8-bf8a6f480adb"},"source":["# 리뷰 문장 추출\n","sentences = test['Sentence']\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1     이 넥타이는 수제품으로 우리나라에서는 하나밖에 없습니다 \n","2            모든 주장이 나름대로의 근거를 갖추고 있다 \n","3                    멍멍한 채로 시간이 흘러갔다 \n","4                만찬 시에는 반주로 마주앙이 나온다 \n","5             아내의 얼굴이 절망적으로 굳어지고 있었다 \n","6           그러나 어머니의 교회행은 그것으로 끝이 났다 \n","7                  그 사실이 진정으로 나는 기뻤다 \n","8                 이런 녀석은 진짜로 혼이 나야 해 \n","9         둘이 말없이 그런 상태로 한참 동안 앉아 있었다 \n","10               처음으로 바깥 풍경이 눈에 들어왔다 \n","Name: Sentence, dtype: object"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"6iDTSbr2s-_N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1596981229371,"user_tz":-120,"elapsed":832,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"4cf47488-8110-4271-f566-9b1de79fb5c5"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 이 넥타이는 수제품으로 우리나라에서는 하나밖에 없습니다  [SEP]',\n"," '[CLS] 모든 주장이 나름대로의 근거를 갖추고 있다  [SEP]',\n"," '[CLS] 멍멍한 채로 시간이 흘러갔다  [SEP]',\n"," '[CLS] 만찬 시에는 반주로 마주앙이 나온다  [SEP]',\n"," '[CLS] 아내의 얼굴이 절망적으로 굳어지고 있었다  [SEP]',\n"," '[CLS] 그러나 어머니의 교회행은 그것으로 끝이 났다  [SEP]',\n"," '[CLS] 그 사실이 진정으로 나는 기뻤다  [SEP]',\n"," '[CLS] 이런 녀석은 진짜로 혼이 나야 해  [SEP]',\n"," '[CLS] 둘이 말없이 그런 상태로 한참 동안 앉아 있었다  [SEP]',\n"," '[CLS] 처음으로 바깥 풍경이 눈에 들어왔다  [SEP]']"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"UyTogb4-tDtE","colab_type":"code","colab":{}},"source":["# 라벨 추출\n","labels = test['Label'].values\n","labels_re = []\n","for label in labels:\n","  labels_re.append(label)\n","labels = labels_re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rH5xfQrtGUD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596981236994,"user_tz":-120,"elapsed":1146,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"178e4393-7e93-46c3-b798-49d17ca4484d"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n","Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"],"name":"stderr"},{"output_type":"stream","text":["[CLS] 이 넥타이는 수제품으로 우리나라에서는 하나밖에 없습니다  [SEP]\n","['[CLS]', '▁이', '▁', '넥', '타', '이', '는', '▁수', '제품', '으로', '▁우리나라', '에서는', '▁하나', '밖에', '▁없', '습니다', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yT5xsMFktKvp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1596981240300,"user_tz":-120,"elapsed":913,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"854b895a-f9ac-4b7b-e2e9-f4859661920e"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   2, 3647,  517, 5705, 7581, 7096, 5760, 2872, 7239, 7078, 3503,\n","       6904, 4928, 6285, 3270, 6701,    3,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"EbA0kPcSGSqp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596981244610,"user_tz":-120,"elapsed":878,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"6874868d-42fa-4b92-bb64-46f0f9a3a4b9"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xinPUSBdGV-p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1596981248528,"user_tz":-120,"elapsed":868,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"1d536616-1e10-43b7-cd2b-fa6233a2d43f"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([   2, 3647,  517, 5705, 7581, 7096, 5760, 2872, 7239, 7078, 3503, 6904,\n","        4928, 6285, 3270, 6701,    3,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0lGakyU6GYM7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596981253197,"user_tz":-120,"elapsed":1050,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"92c659b8-3045-41ce-f5ee-cc91aff71874"},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","test_dataloader"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7f081f1c0828>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"Qne9f9VTn3EK","colab_type":"text"},"source":["#4.Create model"]},{"cell_type":"markdown","metadata":{"id":"2J7g3w4Qn5ib","colab_type":"text"},"source":["##4.1. Load the KoBERT model; BertForSequenceClassification"]},{"cell_type":"code","metadata":{"id":"TcTczNBUuG4a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e8b9429b4cb24fd9affa0f08a091f39d","443fbe3d93fe42d5988e6131350767a7","a1ddff6a093a47a499e1fe2a780d8c24","6b5ef69ea7ff4adf87c9d96a3c0e7c6b","7eedf5cb4881413a8dd0cb9307d15e84","2cc1eb7634a24666969dbf227f9dfb5b","3b2825cda536495b8ebe6b2d9e4774d0","a7ba6ca652c64e3c8f5d271f73fb3bdd","75650f1c394f4223bbc4e7bfbebcb436","a892d0eb4816448b9cf42662149df689","89fc4e5d9de04bc9b49d81bdd01e3f9d","7d2790361558455aa90d0e04b3b316e0","a3976184c7224e298f08e6400c07b5d9","524da2107d4a448e878f19de6ebc77a4","1072cc2b65d4490d9c904d74fca3f7ba","1184a2ebed71498c9b93a907c9082c56"]},"executionInfo":{"status":"ok","timestamp":1596981281997,"user_tz":-120,"elapsed":26925,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"35564cb0-b6d2-428e-a57b-28d6d5a481d4"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"monologg/kobert\", num_labels=6)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8b9429b4cb24fd9affa0f08a091f39d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=426.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75650f1c394f4223bbc4e7bfbebcb436","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=368792146.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"7vT-SSpNn_UI","colab_type":"text"},"source":["##4.2. Create the functions for checking accuracy rate"]},{"cell_type":"code","metadata":{"id":"wJZHor867Ged","colab_type":"code","colab":{}},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","    \n","def FNS_flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    match_num = 0\n","    func_num = 0\n","    for i in range(0,len(pred_flat)):\n","      if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 0):\n","        match_num += 1\n","      if labels_flat[i] == 0:\n","        func_num += 1\n","\n","    if match_num == 0 or func_num == 0:\n","      return 0\n","    else:\n","      return match_num / func_num\n","\n","def INS_flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    match_num = 0\n","    func_num = 0\n","    for i in range(0,len(pred_flat)):\n","      if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 1):\n","        match_num += 1\n","      if labels_flat[i] == 1:\n","        func_num += 1\n","\n","    if match_num == 0 or func_num == 0:\n","      return 0\n","    else:\n","      return match_num / func_num\n","\n","def DIR_flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    match_num = 0\n","    func_num = 0\n","    for i in range(0,len(pred_flat)):\n","      if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 2):\n","        match_num += 1\n","      if labels_flat[i] == 2:\n","        func_num += 1\n","\n","    if match_num == 0 or func_num == 0:\n","      return 0\n","    else:\n","      return match_num / func_num\n","\n","def EFF_flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    match_num = 0\n","    func_num = 0\n","    for i in range(0,len(pred_flat)):\n","      if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 3):\n","        match_num += 1\n","      if labels_flat[i] == 3:\n","        func_num += 1\n","\n","    if match_num == 0 or func_num == 0:\n","      return 0\n","    else:\n","      return match_num / func_num\n","\n","def CRT_flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    match_num = 0\n","    func_num = 0\n","    for i in range(0,len(pred_flat)):\n","      if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 4):\n","        match_num += 1\n","      if labels_flat[i] == 4:\n","        func_num += 1\n","\n","    if match_num == 0 or func_num == 0:\n","      return 0\n","    else:\n","      return match_num / func_num\n","\n","def LOC_flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    match_num = 0\n","    func_num = 0\n","    for i in range(0,len(pred_flat)):\n","      if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 5):\n","        match_num += 1\n","      if labels_flat[i] == 5:\n","        func_num += 1\n","\n","    if match_num == 0 or func_num == 0:\n","      return 0\n","    else:\n","      return match_num / func_num"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vwyfo4xtGpEI","colab_type":"code","colab":{}},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8i8gJgEV0_4Q","colab_type":"text"},"source":["##4.3. Train the corpus data with KoBERT"]},{"cell_type":"code","metadata":{"id":"iMmVMlUPtR6e","colab_type":"code","colab":{}},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 50\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 학습률을 조금씩 감소시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqh2lPAMGqzb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1FpSyV7Cf9yIke6UD822G-yPKCLYPdlHp"},"executionInfo":{"status":"ok","timestamp":1596989598795,"user_tz":-120,"elapsed":8273856,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"29c0276f-1f19-4b34-8503-8271518bf35f"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","final_info = {}\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    FNS_nb_eval_steps, FNS_eval_accuracy = 0, 0\n","    INS_nb_eval_steps, INS_eval_accuracy = 0, 0\n","    DIR_nb_eval_steps, DIR_eval_accuracy = 0, 0\n","    EFF_nb_eval_steps, EFF_eval_accuracy = 0, 0\n","    CRT_nb_eval_steps, CRT_eval_accuracy = 0, 0\n","    LOC_nb_eval_steps, LOC_eval_accuracy = 0, 0\n","\n","    epoch_info = {}\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in test_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","        FNS_tmp_eval_accuracy = FNS_flat_accuracy(logits, label_ids)\n","        FNS_eval_accuracy += FNS_tmp_eval_accuracy\n","        FNS_nb_eval_steps += 1\n","\n","        INS_tmp_eval_accuracy = INS_flat_accuracy(logits, label_ids)\n","        INS_eval_accuracy += INS_tmp_eval_accuracy\n","        INS_nb_eval_steps += 1\n","\n","        DIR_tmp_eval_accuracy = DIR_flat_accuracy(logits, label_ids)\n","        DIR_eval_accuracy += DIR_tmp_eval_accuracy\n","        DIR_nb_eval_steps += 1\n","\n","        EFF_tmp_eval_accuracy = EFF_flat_accuracy(logits, label_ids)\n","        EFF_eval_accuracy += EFF_tmp_eval_accuracy\n","        EFF_nb_eval_steps += 1\n","\n","        CRT_tmp_eval_accuracy = CRT_flat_accuracy(logits, label_ids)\n","        CRT_eval_accuracy += CRT_tmp_eval_accuracy\n","        CRT_nb_eval_steps += 1\n","\n","        LOC_tmp_eval_accuracy = LOC_flat_accuracy(logits, label_ids)\n","        LOC_eval_accuracy += LOC_tmp_eval_accuracy\n","        LOC_nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","    print(\"\")\n","    print(\"  Detail accuracy  \")\n","    print(\"  FNS_Accuracy: {0:.2f}\".format(FNS_eval_accuracy/FNS_nb_eval_steps))\n","    print(\"  INS_Accuracy: {0:.2f}\".format(INS_eval_accuracy/INS_nb_eval_steps))\n","    print(\"  DIR_Accuracy: {0:.2f}\".format(DIR_eval_accuracy/DIR_nb_eval_steps))\n","    print(\"  EFF_Accuracy: {0:.2f}\".format(EFF_eval_accuracy/EFF_nb_eval_steps))\n","    print(\"  CRT_Accuracy: {0:.2f}\".format(CRT_eval_accuracy/CRT_nb_eval_steps))\n","    print(\"  LOC_Accuracy: {0:.2f}\".format(LOC_eval_accuracy/LOC_nb_eval_steps))\n","\n","    epoch_info[\"Total\"] = round(eval_accuracy/nb_eval_steps,3)\n","    epoch_info[\"Loss\"] = round(avg_train_loss,3)\n","    epoch_info[\"FNS\"] = round(FNS_eval_accuracy/FNS_nb_eval_steps,3)\n","    epoch_info[\"INS\"] = round(INS_eval_accuracy/INS_nb_eval_steps,3)\n","    epoch_info[\"DIR\"] = round(DIR_eval_accuracy/DIR_nb_eval_steps,3)\n","    epoch_info[\"EFF\"] = round(EFF_eval_accuracy/EFF_nb_eval_steps,3)\n","    epoch_info[\"CRT\"] = round(CRT_eval_accuracy/CRT_nb_eval_steps,3)\n","    epoch_info[\"LOC\"] = round(LOC_eval_accuracy/LOC_nb_eval_steps,3)\n","\n","    final_info[\"epoch\"+str(epoch_i)] = epoch_info\n","\n","\n","    # 평가모드로 변경\n","    model.eval()\n","    test_input_ids = []\n","    test_input_mask = []\n","    test_labels = []\n","\n","    num = 0\n","    for step, batch in enumerate(test_data):   #467, 128\n","      # print(\"batch\",batch)\n","      # 배치를 GPU에 넣음\n","      batch = tuple(t.to(device) for t in batch)\n","      \n","      # 배치에서 데이터 추출\n","      b_input_ids, b_input_mask, b_labels = batch\n","      input_ids_arr = []\n","      input_mask_arr = []\n","\n","      \n","\n","      for i in range(0,len(b_input_ids)):\n","        input_ids_arr.append(int(b_input_ids[i]))\n","        input_mask_arr.append(int(b_input_mask[i]))\n","\n","      \n","      test_input_ids.append(input_ids_arr)\n","      test_input_mask.append(input_mask_arr)\n","      test_labels.append(int(b_labels))\n","\n","\n","    test_input_ids = torch.tensor(test_input_ids)\n","    test_input_mask = torch.tensor(test_input_mask)\n","    test_labels = test_labels\n","\n","    test_input_ids = test_input_ids.to(device)\n","    test_input_mask = test_input_mask.to(device)\n","\n","\n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(test_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=test_input_mask)\n","        \n","\n","    sentence_vecs_sum = outputs[0]\n","\n","    sentence_array = []\n","    for i in range(0,len(sentence_vecs_sum)):\n","      each_array = []\n","      for j in range(0,len(sentence_vecs_sum[i])):\n","        each_array.append(float(sentence_vecs_sum[i][j]))\n","      sentence_array.append(each_array)\n","\n","    initial_df = pd.DataFrame(sentence_array)\n","\n","    from sklearn.manifold import TSNE\n","    tsne = TSNE(n_components=2, random_state=0)\n","    tsne_obj= tsne.fit_transform(initial_df)\n","\n","    tsne_df = pd.DataFrame({'X':tsne_obj[:,0],'Y':tsne_obj[:,1],'Label':test_labels})\n","\n","    # tsne_df.to_csv(\"drive/My Drive/BERT/SM/KoBERT/Postposition/Data/t-SNE/Lo_tSNE_epoch_\"+str(epoch_i)+\".csv\")\n","\n","    import numpy as np   \n","    import pandas as pd \n","    from plotnine import *\n","\n","    print(\"\")\n","    print(\"  Network visualization  \")\n","    print(ggplot(tsne_df, aes(x='X', y='Y')) + geom_point(aes(colour = 'Label')))\n","\n","print(\"\")\n","print(\"Training complete!\")\n","print(\"\")\n","print(\"Final result is below!\")\n","print(final_info)\n","\n","# f = open(\"drive/My Drive/BERT/SM/KoBERT/Postposition/Data/accuracy/Lo_accuracy_epoch_\"+str(epochs)+\".txt\", 'w')\n","# f.write(str(final_info))\n","# f.close()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"RqIzfB_f8tx2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596989653666,"user_tz":-120,"elapsed":2981,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"fd31e06d-6c43-4792-f9a2-80af19ffad91"},"source":["model.save_pretrained('drive/My Drive/BERT/SM/KoBERT/Postposition/Model/')\n","tokenizer.save_pretrained('drive/My Drive/BERT/SM/KoBERT/Postposition/Model/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('drive/My Drive/BERT/SM/KoBERT/Postposition/Model/tokenizer_78b3253a26.model',\n"," 'drive/My Drive/BERT/SM/KoBERT/Postposition/Model/vocab.txt',\n"," 'drive/My Drive/BERT/SM/KoBERT/Postposition/Model/special_tokens_map.json',\n"," 'drive/My Drive/BERT/SM/KoBERT/Postposition/Model/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"wf_rd7hRoDJe","colab_type":"text"},"source":["- additional test with test_dataloader"]},{"cell_type":"code","metadata":{"id":"Z5_2xdejuFkg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596989662798,"user_tz":-120,"elapsed":7545,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"96616818-cb42-4dc3-967e-03a4dc71e15f"},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):  #15\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","        \n","    # 로스 구함\n","    logits = outputs[0]  #32 , 8\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Accuracy: 0.85\n","Test took: 0:00:07\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3V1EWW9ioHVF","colab_type":"text"},"source":["#5.Create function using trained model"]},{"cell_type":"markdown","metadata":{"id":"2plKAkJXoOE_","colab_type":"text"},"source":["##5.1. Create functions"]},{"cell_type":"code","metadata":{"id":"Xl9u8FQAxtOs","colab_type":"code","colab":{}},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhEpJDl2xw9R","colab_type":"code","colab":{}},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vA29TR7W19zd","colab_type":"text"},"source":["##5.2. Check the result with real sentence"]},{"cell_type":"code","metadata":{"id":"lrFhwBdvxzVr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596989671587,"user_tz":-120,"elapsed":1016,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"6c898392-16b0-4d98-e173-445893634b85"},"source":["logits = test_sentences(['나는 집으로 가고 있습니다.'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-2.5875225 -1.795376   9.1881895 -2.2699857 -2.0289989 -1.7857274]]\n","2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fh2cKmxHx2Qz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596989673952,"user_tz":-120,"elapsed":855,"user":{"displayName":"Seongmin Mun","photoUrl":"","userId":"17295717412647902050"}},"outputId":"58130ddc-6d5b-414f-abed-3f385180831d"},"source":["logits = test_sentences(['나는 가위로 종이를 잘랐습니다.'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-1.6707162  8.955514  -1.5057735 -1.4657849 -1.4273801 -1.6496873]]\n","1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1zTXiZ1uMsN5","colab_type":"text"},"source":["#6.Conclusion"]},{"cell_type":"markdown","metadata":{"id":"k4acOBfeMxsY","colab_type":"text"},"source":["##6.1. Summary\n","- "]},{"cell_type":"markdown","metadata":{"id":"f1fCTS6IhTjy","colab_type":"text"},"source":["##6.2. Suggestion\n","- "]}]}